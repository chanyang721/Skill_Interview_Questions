> DB User 테이블 정규화 연습

- 기존의 유저 테이블과 관련된 Agency, Enterprise, School, 그리고 ConsultingConnections, ConsultingReplies 테이블에 대한 1 ~ 3차 정규화 적용하는 연습을 해보았다. 내가 생각하는 정규화의 목적은 데이터의 중복 입력을 최소화 함과 동시에 SQL문 작성 시 반응의 낭비 없이 하나의 릴레이션으로 만들어 내는 것이 가능한 구조라고 생각된다.

- 앞서 나열된 테이블들은 정규화를 배우기 전에 만들었던 테이블이지만, 내가 !!?? 생각만큼 엉망으로 만들지는 않았었다. 2차 정규화까지 어느정도 만족하는 테이블들이었다.
- 우선 User테이블에서 consulting_qualification컬럼을 원자 단위로 하지 않았기 때문에, 원자 단위로 저장 할 수 있는 테이블을 새로 생성하였다. 저렇게 만든 이유로는 우선 컨설턴트들의 자격들을 단순하게 입력만 하면 되었기 때문이다. 하지만 일단 테이블로 뺴놔야 쿼리문에서 사용하는 것이 가능하고, 추 후에 사용자가 특정 자격이 있는 컨설턴트를 검색할 수 있기 때문에 테이블로 빼두는 것이 가장 좋을 것 같다.
- 그 후, 2차 정규화는 완전 함수 종속(기본키[PK]의 후보키들이 결정자가 되어서는 안된다)을 제거하는 과정으로 이 부분은 ConsultingConnections 테이블에서 찾을 수 있었다. ConsultingConnections에서 A컨설턴트가 B컨설티드 유저와 여러번 매칭이 되는 경우 데이터의 반복이 발생하게 된다. 그렇기 때문에 매칭되는 컨설턴트-컨설티드 유저의 연결 테이블과 해당 연결이 반복되더라도 발생되는 데이터의 중복이 발생하게 된다. 이를 해결하기 위해 ConsultingConnections 테이블에는 매칭 관계만 두고, 해당 테이블 내에 존재하는 Consulting Information을 다른 테이블로 생성하여 반복을 최소화 하려한다.
- 마지막으로 3차 정규화는 이행적 종속 과정 (A → B and B → C이면 A → C이면 안된다)인 관계가 있는 테이블을 살펴보고 있는데 현재는 정확하게 이해하지 못해서 찾아내지 못하고 있는 것 같다. 내일은 할 일이 있어서 주말에 다시 공부 할 예정이다.

> Redshift 스케줄러 생성 과정 블로깅

- Redshift로 S3에 지속적으로 업데이트 되는 csv파일들을 copy 명령어로 가져오는 쿼리문을 스케줄러를 통해 가져올 수 있다는 것을 알았다.
- 처음에 S3에 업로드 되는 csv파일을 어떻게 하면 가져올 수 있을지 구글링 해본 결과 AWS Glue와 AWS Data Pipeline을 찾을 수 있었다. 하지만 DataPipeline은 서울 리전에서 서비스중인 상태가 아니였기 때문에 도쿄 리전에서 시도해봤는데 서비스 자체가 조금 안정화 되지 않은 상태이기도 하고 어떻게 해야하는지에 대한 설명이 구글 상에서도 너무 찾기 어려웠다. 그래서 다른 서비스를 알아본게 Glue였지만, Redshift 서비스 상에서 쿼리를 작성하고 스케줄러까지 설정하는 것이 가능하다는 것을 알게 되었다.
- 스케줄러를 실행하기 위해 다행이도 AWS에 자세한 설명이 있었지만, 정책을 연결하는 과정에서 생각보다 애를 먹었다. 사용자 계정에서 권한 문제가 있어서 root 계정으로 들어가 시도를 하면서 발생하게 되었는데, 결론만 말하자면 root유저로 접속해서 다른 서비스를 사용하는 것을 지양하게 하기 위해 AWS 상에서 root유저로 권한?에 접근하는 것을 막아둔 것 같다. 결국 구글에서 stack overflow에서 해결책을 찾게 되었는데 root유저로 서비스 권한에 접근하는 것이 불가능하다라는 언급이 있었다. 특히 STS 에러를 만났는데 기존에 세팅한 상태로 사용자 계정으로 돌아가 다시 시도해 보니 그냥.. 되었다.
- AWS의 정책 상 root 유저는 사용자 계정을 생성하여 권한을 부여하는 역할만 하도록 하는 것 같다.
